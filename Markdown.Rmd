title: "Veri Tipi Belirleme"
author: "Bükra Doğaner"
date: '2022-03-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

çalışacağımız verisetinin içeri aktarılması için `read.csv`` fonksiyonu kullanılır. 
içeri aktarcağımız lokal dosya yolu yada uzak sunucuda tutulan dosyanın yolu fonksiyona girdi olarak verilir. 

```{r}
veriseti <- read.csv("fin500.csv")
```

veriseti yüklendikten sonra veriseti içeriğini kontrol etmek üzere `head()` ve `tail()` fonksiyonları kullanılır


```{r}
head(veriseti,15)

tail(veriseti,2) 
```
verisetinde yer alan dataların yapısını görmek için str fonksiyonu kullanılır

```{r}
str(veriseti) 
```

Veriseti içindeki tek bir stünün okunması için veriseti$xx komutu kullanılır. Örnek olarak Industry stünunun görüntülenmesi için 

```{r}
veriseti$Industry
```

Verisetine dair özet bilgi için summary() fonksiyonu kullanılır.

```{r}
summary(veriseti) 
```

verisetinde yer alan verilerin yapısının değiştirilmesi için veriseti$() <- as.factor(veriseti$veriseti$değişiklik yapılacak sütun) fonksiyonu kullanılır. Verilerin doğru analiz edilmesi için verisetinde yer alan bilgilerin doğru şekilde tanımlanması gerekmektedir. Tüm veriler tek tek incelenerek tanımlaması yapılmalıdır.

```{r}
veriseti$Industry <- as.factor(veriseti$Industry)
veriseti$Inception <- as.factor(veriseti$Inception)
veriseti$State <- as.factor(veriseti$State)
veriseti$City <- as.factor(veriseti$City)
veriseti$Revenue <- as.character(veriseti$Revenue)
```

Verilerin istenilen formata uygun olması için 'veri temizliği' işlemi yapılmalıdır. Bu işlemde gsub() komutu kullanılabilmektedir. Verilerin rakamsal veri olarak algılanması için sayıların sonunda yer alan dolar kelimesinin silinmesi fonksiyonu aşağıdaki gibidir. gsub("çıkarılması gereken değer", "yerine konulacak değer", veriseti$değişiklik yapılacak sütun)

```{r}
veriseti$Expenses <- gsub(" Dollars", "", veriseti$Expenses)
veriseti$Expenses <- gsub(",", "", veriseti$Expenses) 
``` 

işlem kalabalığı olmaması açısından gsub fonksiyonunun kullanıldığı işlemi aşağıdaki şekilde birleştirmek mümkündür. 

```{r}
veriseti$Growth <- as.numeric(gsub("%", "", veriseti$Growth))
``` 

Verilerde yer alan $ gibi R programında bir anlam ifade eden karakterlerle ilgili işlem yapılacağı zaman başına \\ koyularak karakterin programda bir anlam ifade etmediği tanımlanır.

```{r}
veriseti$Revenue <- gsub("\\$","", veriseti$Revenue)
``` 


verilerin son durumu aşağıdaki gibidir.

```{r}
str(veriseti)
```

"İkinci Ders"

alttaki scale formülü
$$  
\frac{X{i}}{\sigma{x}}
$$
Yeni veri seti ile çalışmaya başladık
Matematiksel işlemler ile çalışacağımızdan dolayı `caret` paketini yükledik. Daha sonra R'da yerleşik veri kümesi olan `iris` veri kümesini çağırdık ve `iris` veri kümesinin yapısını ve özetini ortaya koyduk.
```{r}
library(caret)
data(iris)
str(iris)
```
#verisetindeki tüm satırlar ile 1. ve 4. sütunlar arasını aldık. Eğer 1. 3. ve 5. sütunları almak isteseydik C(1,3,5) komutunu uygulamamız gerekecekti.
```{r}
summary(iris[,1:4])
```
Verisetimizde uygulayacağımız dönüşümler (matematiksel işlemler) için `preProcess` komutunu kullandık ve `preProcess` altındaki `scale` paketini çağırdık. Scale fonksiyonu her bir gözlemi o serinin standart sapmasına göre oranlamaktadır.
```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("scale"))
print(preProcessParams)
```
$$
\frac{x_(i)}{\sigma_(x)}
$$
İşlemleri belirli veri kümelerine uygulamak için `predict` komutunu kullandık. Sonrasında veri setini görmek için summary fonksiyonu kullandık.
```{r}
scaled <- predict(preProcessParams, iris[,1:4]) #predict fonksiyonunu dönüştürerek iris 1:4 e yazdırdık.
summary(scaled)
```
`preProcess` altındaki `scale` paketini çağırdık ve dönüşüm yaparak yazdırdık. Center fonksiyonu her bir değişkenin ortalamalarından çıkartılmış halini vermektedir.

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("center")) #xi-xmü yaptık
print(preProcessParams)

centered <-  predict(preProcessParams, iris[,1:4])
summary(centered)
```
$$
x_{i}-\bar{X}_{x}
$$
Standardizasyon uygulayabilmek için;
```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("center", "scale"))
print(preProcessParams)

standardized <- predict(preProcessParams, iris[,1:4])
summary(standardized)
```
$$
\frac{x_{i}-\mu{x}}{\sigma_{x}}
$$

Normalizasyon uygulayabilmek için;

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("range")) 
print(preProcessParams)
normalized <- predict(preProcessParams, iris[,1:4])
summary(normalized)
```

$$
\frac{x-min\left(x \right )}{max\left(x \right )-min\left(x \right )}
$$
`boxcox` dönüşümü, normal olarak dağıtılmamış bir veri kümesini daha normal dağılmış bir veri kümesine dönüştürmek için yaygın olarak kullanılan bir yöntemdir. `boxcox` uygulayabilmek için;
```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("BoxCox"))
print(preProcessParams)

boxcox <- predict(preProcessParams, iris[,1:4])
summary(boxcox)
```

$$
y\left( \lambda \right )=\left\{\begin{matrix}\frac{y^ \lambda -1 }{\lambda}, & if \lambda\neq 0\\ log\left (y  \right ), & if \lambda = 0 \end{matrix}\right.
$$

`Yeo-Johnson` dönüşümü, normalleştirme dönüşümünü hesaplar. `yeojohnson` uygulabilmek için;

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("YeoJohnson"))
print(preProcessParams)

yeojohnson <- predict(preProcessParams, iris[,1:4])
summary(yeojohnson)
```
$$
\psi \left( \lambda, y \right )=

\left\{\begin{matrix}

({(\lambda +1)^\lambda -1)/\lambda}, & if \lambda\neq 0, y\geq 0\\

log\left (y+1  \right ), & if \lambda = 0, y\geq 0 \\ 

-[(-y+1)^{2-\lambda }-1)]/(2-\lambda), & if \lambda \neq  2, y< 0 \\ 

-log\left (-y+1  \right ), & if \lambda = 2, y< 0\end{matrix}\right.
$$
Resampling yani yeniden örnekleme yapacağız. Bunun için Caret Package yükledik ve verimizi çağırdık.
```{r}
library(caret) #load caret package
data(iris) #Load the dataset
```
Bunun için Boolstrap yöntemi kullandık ve verilerden yeniden 100 verilik örneklem oluştuduk.
```{r}
train_control <- trainControl(method="boot", number = 100)
```
Veri setinde %80/%20 eğitim/test bölünmesini tanımladık
```{r}
split=0.80
trainIndex <- createDataPartition(iris$Species, p=split, list=FALSE)
```
80/20 olarak böldüğümüz verinin 80lik kısımını yani eğitim datasını çağırdık. 5 fold oluştuğunu gördük.  Veriyi her taraftan inceleyerek en uygun test datası nerden elde edilir onu hesaplamaya çalıştık.
```{r}
data_train <- iris[trainIndex,]
```
20'lik kısmı görmek için ise formülde başına eksi koyduk. 
```{r}
data_test <- iris[-trainIndex,]
```
Çapraz doğrulama yaptık. One Out modelini seçtik. Bu bir model doğrulama tekniğidir. Eğittiğimiz modelin amacı doğrultusunda ne kadar düzgün şekilde ilerlediğini, hatalar olup olmadığını, kontrol etmek için kullanılır. Eğitim kontrolünü tanımlattık..
```{r}
# Leave One Out Cross Validation (LOOCV)
library(caret)
#load the iris dataset
data(iris)
#define training control
train_control <- trainControl(method="LOOCV")
```

Ayrıca bir de K-Fold Cross Validation uyguladık. Bu yöntem de sınıflandırma modellerinin değerlendirilmesi ve modelin eğitilmesi için veri setini parçalara ayırma yöntemlerinden biridir. Burada 10 fold oluşturduk.
```{r}
#load the library
library(caret)
#load the iris dataset
data(iris)
#define training control
#k fold number => number
train_control <- trainControl(method="cv", number=10)
```
Lineer regresyon uygulaması yapmaya başladık. Regresyonu R programında tanımlı 'cars' paketi üzerinden uygulamak için veriyi çağırdık ve veriyi inceledik.
```{r}
head(cars)
str(cars)
summary(cars)
```
Verideki kolon isimlerine bakmak için 'names' fonksiyonunu kullandık. Bu fonksiyon Veri setindeki 'header'ları gösterir.
```{r}
names(cars)
```
Verilerin regresyon modeli kurmaya uygun olup olmadığını kontrol etmek için saçılma diyagramı çizdirdik. Aynı şekilde bu amaçla değişkenler arasındaki korelasyona baktık.
```{r}
scatter.smooth(x=cars$speed, y=cars$dist, main="Saçılma Diyagramı")
cor(cars$speed, cars$dist)
```
Veri üzerinde lm fonksiyonu ile lineer regresyon modeli kurduk. Bu modelde distance bağımlı değişkenken speed bağımsız değişken olarak yer aldı. Daha sonra bu modeli yazdırdık ve özetine baktık.
```{r}
GenelModel <- lm(dist ~ speed, data=cars)
print(GenelModel)
summary(GenelModel)
```
Akaike Bilgi Kriteri ve Bayesian Bilgi Kriteri değerine baktık.AIC herhangi bir tahmini istatistiksel modelin uygunluğunun bir ölçüsü olarak adlandırılmaktadır.BIC ise farklı parametre sayılarına sahip parametrik modeller sınıfı arasında bir model seçimi türüdür. Bu iki değerin de küçük olması istenmektedir.

```{r}
AIC(GenelModel)
BIC(GenelModel)
```
Makine öğrenmesi bu bölüm ve sonrasında uygulanmıştır. Rastgele örnekleme sonuçlarını çoğaltmak için seed(tohum) ayarladık. Hepimiz aynı rassal sayıları bulmak için değer olarak 100 yazdık. Daha sonra verilerin %80'i ile 1'den n'e kadar index değerleri ürettik.
```{r}
set.seed(100)
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))
```
Veri setini bölerek eğitim ve test verisi olarak parçaladık. Model eğitim verisini ve test verisini oluşturduk. Test verisi oluşturuken formülasyonda başa '-' işareti ekledik.
```{r}
trainingData <- cars[trainingRowIndex, ] 
View(trainingData)
testData <- cars[-trainingRowIndex, ] 
View(testData)
```
Eğitim verisi üzerinden yeniden model kurduk. Modelde distance bağımlı değişken, speed bağımsız değişken olarak yer aldı.
lmmod fonksiyonunu kullandık. Bu fonksiyonu kullanarak model test verisindeki 'speed' değişkenlerini modele koyarak 'distance' değişkenlerini tahmnin etmekte. Bu tahmin edilen verileri distPred olarak adlandırdık.
Oluşan verilerin özetine baktık ancak çok anlamlı olmadığını gördük. Ardından yine Akaike Bilgi Kriteri değerini kontrol ettik. Değerin biraz kötülerştiğini gördük.
```{r}
lmMod <- lm(dist ~ speed, data=trainingData)
distPred <- predict(lmMod, testData)
print(distPred)
summary(lmMod)
AIC(lmMod)
```
Gerçekleşen verileri ve model ile tahmin edilen verileri net görebilmek için yanyana yazdırdık.
Gerçekleşen veriler ile tahmin edilen verilerin ne kadar örtüştüğünü görmek amacıyla korelasyonlarına baktık.
```{r}
actuals_preds <- data.frame(cbind(gercek=testData$dist, tahmin=distPred))
correlation_accuracy <- cor(actuals_preds)
print(correlation_accuracy)
head(actuals_preds)
```
#min_max_accuracy <- mean(apply(actuals_preds), 1, min) / apply(actuals_preds, 1, max)) 
MAPE yani mean absolute percentage error değerine baktık. Bu değer ortalama hata bir makine öğrenmesi modelinin öngördüğü tahmin değerleri ile gerçek değerlerin arasındaki ortalama hatadır.
Yazdırarak modelin %50 hata yapmış olduğunu gördük
```{r}
mape <- mean(abs((actuals_preds$tahmin - actuals_preds$gercek))/actuals_preds$gercek)
print(mape)
```
Modelde makine öğrenmesi kısmı asıl burada başladı.
Bunun için kütüphanden DAAG paketini yükledik.
CVlm (cross validation lm) fonksiyonu DAAG paketi yüklenince gelmektedir. Burada form formülasyonu linear modeln formülasyonunu, m ise modelin oluşturmasını istediğimiz küme sayısını belirtmektedir. Burada  yazarak Veri setini 5 parçaya böldürmüş ve 5 ayrı regresyon modeli oluşturmuş olduk. Bu 5 parça içerisinde test verisi sürekli değişti. Bu sayede makine öğrenmesi yapmış olduk.
Daha sonra MSE yani mean squarred error (ortalama hata karesi) değerine baktık.Bu hata bir tahmincinin ortalama karesi alınmış hatası veya ortalama karesi alınmış sapma, hataların karelerinin ortalamasını, yani tahmin edilen değerler ile gerçek değer arasındaki ortalama karesi alınmış farkı ölçmektedir.
```{r}
library(DAAG)
cvResults <- suppressWarnings(CVlm(cars, form.lm = dist ~speed, m=5, dots = FALSE, seed=29, legend.pos ="topleft", printit = FALSE, main="CV Dogrusal Regresyon"));
attr(cvResults, 'ms')
```
Ridge Regresyon, Lasso Regresyon ve Elastic Net Regressyon uygulaması yapıldı. Bunun için öncelikle kullanılacak tidyverse, caret, glmnet ve MASS paketleri indirildi ve çağırıldı. Uygulamada kullanılacak Boston veri seti çağırıldı.
```{r}
library(tidyverse)
library(caret)
library(glmnet)
data(Boston, package = "MASS")
```
Rastgelelik içeren işlemlerde herkesin aynı sonucu bulabilmesi için set.seed fonksiyonu kullanıldı.Test ve eğitim verileri rastsallık ile oluşturuldu. Bu aşamada örneklemin %75'i alındığında küsüratlı bir rakam çıktığından floor fonksiyonu kullanılarak tam sayı olması için yuvarlama yapıldı. Bu noktada vektör kullanarak dizinler oluşturmak için seq_len fonksiyonu kullanıldı. Ardından eğitim datası ve sonrasında - koyularak test datası oluşturuldu.
```{r}
set.seed(1212)
sample_size <- floor(0.75 * nrow(Boston))
training_index <- sample(seq_len(nrow(Boston)), size =sample_size)
train <- Boston[training_index, ]
test <- Boston[-training_index, ]
```
model.matrix fonksiyonu kullanılarak bir model matrisi oluşturuldu. Belirtilen formül ve verilerle regresyon benzeri bir model için bir model tasarım matrisi yapılmış oldu. Bunu yaparken median value bağımlı değişken diğerleri ise  bağımsız değişken olarak alındı. Bağımsız değişkenler yerine "." koyarak hepsi alınmış olundu.
```{r}
x <- model.matrix(medv~., train)[,-1]
y <- train$medv
```
Ridge Regression uygulaması yapıldı. Bu regresyon modelinin amacı hata kareler toplamını minimize eden katsayıları, bu katsayılara bir ceza uygulayarak bulmaktır ve formülü aşağıda yer almaktadır.
$$
\begin{equation*} \sum_{i=1}^n (y_i - \sum_{j=1}^p x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^p \beta_j^2 \end{equation*}
$$
cv.glmnet fonksiyonu kullanıldı. Bu İşlev glmnet için k-fold çapraz doğrulama yapar , bir çizim oluşturur ve lambda için bir değer döndürür. Burada x bağımlı değişken, y bağımsız değişken olarak alındı ve alfa=0 olarak girildi.Alfa=0 ridge regresyon olduğu için kullanılmaktadır.
```{r}
cv.r <- cv.glmnet(x, y, alpha = 0)
```
Cross validation fonksiyonu ile lambda'nın minimum değeri bulundu. Sonrasında bulunan bu min lambda değeri fonksiyona elle yazıldı ve girdi olarak kullanıldı.
```{r}
cv.r$lambda.min 
model.ridge <- glmnet(x, y, alpha = 0, lambda = cv.r$lambda.min)
```
Modelin coefficient değerlerine yani katsayılarına bakıldı.
```{r}
coef(model.ridge) 
```
Belirtilen formül ve verilerle regresyon benzeri bir model için tasarım matris oluşturuldu. Bunun için test datası kullanıldı. Model predict değerleri oluşturdu ve bulduklarını vektör olarak kullandı.
```{r}
x.test.ridge <- model.matrix(medv ~., test)[,-1]
predictions.ridge <- model.ridge %>% predict(x.test.ridge) %>% as.vector() 
```
Bu şekilde oluşturulan tahmin değereri model verileri ile uyuşuyor mu diye kontrol edilmesi için RMSE fonksiyonu kullanıldı.
```{r}
data.frame( RMSE.r = RMSE(predictions.ridge, test$medv), Rsquare.r = R2(predictions.ridge, test$medv))
```
Lasso Regression uygulaması yapıldı. Bu model Ürettiği modelin tahmin doğruluğunu ve yorumlanabilirliğini arttırmak için hem değişken seçimi hem de regularization yapmaktadır. Aynı ridge regresyonda olduğu gibi amaç hata kareler toplamını minimize eden katsayıları, katsayılara ceza uygularayarak bulmaktır. Fakat ridge regresyondan farklı olarak ilgisiz değişkenlerin katsayılarını sıfıra eşitlemektedir.Ridge regresyondan farkının ormülü aşağıda yer almaktadır.
$$
\begin{equation*} \textrm{Lasso subject to:} \sum_{j=1}^p |\beta_j| < c. \end{equation*}
$$
cv.glmnet fonksiyonu kullanıldı. Bu İşlev glmnet için k-fold çapraz doğrulama yapar , bir çizim oluşturur ve lambda için bir değer döndürür. Burada x bağımlı değişken, y bağımsız değişken olarak alındı ve alfa=1 olarak girildi.Alfa=1 ridge resgreyondan farklı olarak lasso regresyon için kullanılmaktadır.
```{r}
cv.l <- cv.glmnet(x, y, alpha = 1)
```
Cross validation fonksiyonu ile lambda'nın minimum değeri bulundu. Sonrasında bulunan bu min lambda değeri fonksiyona elle yazıldı ve girdi olarak kullanıldı.
```{r}
cv.l$lambda.min
model.lasso <- glmnet(x, y, alpha = 1 , lambda = cv.l$lambda.min)
```
Modelin coefficient değerlerine yani katsayılarına bakıldı.
```{r}
coef(model.lasso)
```
Belirtilen formül ve verilerle regresyon benzeri bir model için tasarım matris oluşturuldu. Bunun için test datası kullanıldı. Model predict değerleri oluşturdu ve bulduklarını vektör olarak kullandı.
```{r}
x.test.lasso <- model.matrix(medv ~., test)[,-1] 
predictions.lasso <- model.lasso %>% predict(x.test.lasso) %>% as.vector()
```
Bu şekilde oluşturulan tahmin değereri model verileri ile uyuşuyor mu diye kontrol edilmesi için RMSE fonksiyonu kullanıldı.
```{r}
data.frame( RMSE.r = RMSE(predictions.lasso, test$medv), Rsquare.r = R2(predictions.lasso, test$medv))
```
Son olarak Elastik Net uygulaması yapıldı. Elastik Net Regresyonu, Lasso ve Ridge Regresyonlarının güçlü yönlerini birleştirerek, düzenlileştirilmiş değişkenlerle ilişkili parametreleri gruplandırıp küçülterek onları denklemde bırakmakta veya hepsini bir kerede kaldırmaktadır. 
```{r}
model.net <- train(
  medv ~., data = train, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLenght = 10)
model.net$bestTune
```
Modelin coefficient değerlerine yani katsayılarına bakıldı.
```{r}
coef(model.net$finalModel, model.net$bestTune$lambda)
```
Belirtilen formül ve verilerle regresyon benzeri bir model için tasarım matris oluşturuldu. Bunun için test datası kullanıldı. Model predict değerleri oluşturdu ve bulduklarını vektör olarak kullandı. Modelde, Alpha'nın % 20, Ridge Regresyon Modeli'nin, % 80 katkısı olacağı görüldü.
```{r}
x.test.net <- model.matrix(medv ~., test)[,-1] 
predictions.net <- model.net %>% predict(x.test.net)
```
Bu şekilde oluşturulan tahmin değereri model verileri ile uyuşuyor mu diye kontrol edilmesi için RMSE fonksiyonu kullanıldı.
```{r}
data.frame( RMSE.r = RMSE(predictions.net, test$medv), Rsquare.r = R2(predictions.net, test$medv))
```
Genelleştirilmiş lineer modeller uygulandı. İlk olarak Geleneksel Model ile başlandı. Bunun için "Mortgage" veri seti ile çalışıldı. İlk etapta yüklendi ve özellikleri incelendi.
```{r}
dataset = read.csv("Mortgage.csv")
head(dataset)
str(dataset)
```
Sütun türü numerik yerine factor olarak tanımlandı.
```{r}
dataset$y = as.factor(dataset$y)
```
Model kuruldu. Daha sonra olasılık değerleri elde etmek için burada elde edilen x1 ve x2 değerleri kullanılmaktadır. Modelde tüm değerler <0.05 olduğu için genel olarak anlamlı olduğu söylenebilir.
```{r}
model1= glm(y~x1+x2, family = "binomial", data = dataset)
summary(model1)   
```
R2 değerini görmek için ll.null fonksiyonu kullanıldı. Bu formül sistem en kötü haliyle tahmin yaparsa oluşacak sonuçları göstermektedir. ll.proposed fonksiyonu ise en iyi tahmini veren denklemin sigmoidini görmek için kullanıldı. R2 değeri %46 çıkmıştır, bu değer bağımlı değişkenin ne kadar varyasyonunun bir regresyon modelindeki bağımsız değişkenler tarafından açıklandığını göstermektedir. Sonrasında null ve residual deviance kullanılarak Akaike hesaplanacaktır.
```{r}
ll.null = model1$null.deviance/-2
ll.proposed = model1$deviance/-2
R2 = (ll.null - ll.proposed) / ll.null
```
Buraya kadar yapılan işlemlerde oluşturulan model test edilmiş oldu. Buradan sonrasında model tahminlemede kullanılacaktır. Round fonksiyonu ile tahmin değerleri %50'nin altında veya üstünde olma durumuna göre 0 veya 1'e yuvarlanmıştır.
```{r}
tahmin = predict(model1, type="response")
classification = round(tahmin) 
```
AR fonksiyonu ile doğrulama yapıldı. Modelin %86 doğru, %14 yanlış tahmin yaptığı görüldü.
```{r}
AR = mean(dataset$y == classification)
AR = 100*mean(dataset$y == classification) 
```
İkinci olarak train test datası ile model oluşturuldu. Bunun için "Spam" veri seti çağırıldı ve özelliklerine bakıldı. Spam bağımlı değişken olarak kullanıldı. Sütun türü numerik yerine factor olarak tanımlandı. Herkesin aynı sonuçları alması için seed oluşturuldu.
```{r}
veriseti = read.csv("Spam.csv")
veriseti$Spam = as.factor(veriseti$Spam)
str(veriseti)
set.seed(1453)
```
Nrow satır sayısını ifade etmektedir. Veri setinin yüzde seksenini alcak 1den nrowa kadar indisler oluşturuldu. Test ve eğitim verileri hazırlandı.
```{r}
trainingRowIndex = sample(1:nrow(veriseti), 0.8*nrow(veriseti)) #nrow satır sayısı
train = veriseti[trainingRowIndex,2:5]  #record almadık
test = veriseti[-trainingRowIndex,2:5]
```
Sonraki aşamada model kuruldu. ll.null fonksiyonu kullanıldı. Bu formül sistem en kötü haliyle tahmin yaparsa oluşacak sonuçları göstermektedir. ll.proposed fonksiyonu ise en iyi tahmini veren denklemin sigmoidini görmek için kullanıldı.
```{r}
model2 = glm(Spam~., family="binomial", data = train)

ll.null = model2$null.deviance/-2
ll.proposed = model2$deviance/-2
```
R2 değeri hesaplandı. %30 olarak bulundu. Bu değer bağımlı değişkenin ne kadar varyasyonunun bir regresyon modelindeki bağımsız değişkenler tarafından açıklandığını göstermektedir.
```{r}
R2 = (ll.null - ll.proposed) / ll.null  #modelin R2'si %30
```
Buraya kadar yapılan işlemlerde oluşturulan model test edilmiş oldu. Buradan sonrasında model tahminlemede kullanılacaktır. Round fonksiyonu ile tahmin değerleri %50'nin altında veya üstünde olma durumuna göre 0 veya 1'e yuvarlanmıştır.
```{r}
tahmin2 = predict(model2, test, type="response")
classification = round(tahmin2)
```
AR fonksiyonu ile doğrulama yapıldı. Test içindeki spamlarle classificationlar ne kadar örtüştüğüne bakıldı. Doğru tahmin yüzdesi %83 olarak bulundu.
```{r}
AR = 100*mean(test$Spam == classification)  #%83
```
Üçüncü ve son olarak K-fold cv train-test yöntemi kullanıldı. Bunun için "caret" paketi çağırıldı. Kullanılacak olan 2 ve 5 arasındaki kolanlar çağırıldı.
```{r}
library(caret)
veriseti = read.csv("Spam.csv")
veriseti = veriseti[ , 2:5]
```
Sütun türü numerik yerine factor olarak tanımlandı. Herkesin aynı sonuçları alması için seed oluşturuldu.
```{r}
veriseti$Spam = as.factor(veriseti$Spam)
str(veriseti)
```
Veri seti 5 parçaya bölündü ve model oluşturuldu.
```{r}
myControl = trainControl(method = "cv", number = 5)

model3 = train(Spam~., data=veriseti, trcontrol=myControl,
              method = "glm", family=binomial,
              metric = "Accuracy")
